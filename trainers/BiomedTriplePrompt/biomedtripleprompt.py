"""
BiomedTriplePrompt Trainer
==========================
ä¸‰å±‚çº§æ¢¯åº¦å¼ Prompt å­¦ä¹ ï¼šé«˜è´¨é‡ï¼ˆæ•™å¸ˆï¼‰â†’ ä¸­ç­‰è´¨é‡ï¼ˆå­¦ç”Ÿ1ï¼‰â†’ ä½è´¨é‡ï¼ˆå­¦ç”Ÿ2ï¼‰

æ ¸å¿ƒæœºåˆ¶:
1. é«˜è´¨é‡ Promptï¼ˆæ•™å¸ˆï¼Œå†»ç»“ï¼‰ï¼šGPT-4 ç”Ÿæˆçš„å®Œæ•´ä¸´åºŠæè¿°
2. ä¸­ç­‰è´¨é‡ Promptï¼ˆå­¦ç”Ÿ1ï¼Œå¯å­¦ä¹ ï¼‰ï¼šCUSTOM_BIOMEDDPT_TEMPLATES å›ºå®šæ¨¡æ¿
3. ä½è´¨é‡ Promptï¼ˆå­¦ç”Ÿ2ï¼Œå¯å­¦ä¹ ï¼‰ï¼šç±»åˆ«åæˆ–ç©ºæç¤º

çŸ¥è¯†è’¸é¦è·¯å¾„:
- é«˜è´¨é‡ â†’ ä¸­ç­‰è´¨é‡ï¼šç»†ç²’åº¦è¯­ä¹‰ä¼ é€’
- é«˜è´¨é‡ â†’ ä½è´¨é‡ï¼šå¼ºåˆ°å¼±çš„é²æ£’æ€§å­¦ä¹ 
- ä¸­ç­‰è´¨é‡ â†’ ä½è´¨é‡ï¼šä¸­é—´å±‚è¯­ä¹‰æ¡¥æ¥

æ–‡ä»¶ä½ç½®ï¼štrainers/BiomedTriplePrompt/biomedtripleprompt.py
"""

import os
import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.cuda.amp import GradScaler, autocast

from dassl.engine import TRAINER_REGISTRY, TrainerX
from dassl.metrics import compute_accuracy
from dassl.utils import load_checkpoint
from dassl.optim import build_optimizer, build_lr_scheduler

from open_clip import create_model_from_pretrained
from open_clip.tokenizer import tokenize

# å¯¼å…¥ Prompt æ¨¡æ¿
from trainers.prompt_templates import (
    TEMPLATES,  # é«˜è´¨é‡ GPT-4 Promptï¼ˆå·²æœ‰ï¼‰
    ZERO_SHOT_TEMPLATES  # ä½è´¨é‡ Promptï¼ˆæ–°å¢ï¼‰
)


# ========== ä¸­ç­‰è´¨é‡ Prompt æ¨¡æ¿ï¼ˆCUSTOM_BIOMEDDPT_TEMPLATESï¼‰==========
CUSTOM_BIOMEDDPT_TEMPLATES = {
    "BTMRI": "a MR photo of a {} in the brain.",
    "BUSI": "a ultrasound photo of a {} in the breast.",
    "CHMNIST": "a histopathological photo of a {}.",
    "COVID_19": "a chest X-ray photo of a {} affected by COVID-19 in the lung.",
    "CTKidney": "a CT photo of a {} in the kidney.",
    "DermaMNIST": "a dermatoscopy photo of a {} in the skin.",
    "KneeXray": "a frontal X-ray photo of a {} in the knee joint.",
    "Kvasir": "a endoscopic photo of a {} in the colon.",
    "LungColon": "a histopathological photo of a {}.",
    "OCTMNIST": "a OCT photo of a {}.",
    "RETINA": "a photo of a {} presented in image.",
}


def load_biomedclip_to_cpu(cfg):
    """åŠ è½½ BiomedCLIP æ¨¡å‹"""
    print("Loading BiomedCLIP-PubMedBERT_256-vit_base_patch16_224...")
    clip_model, preprocess = create_model_from_pretrained(
        'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224',
        cache_dir='clip/checkpoints/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'
    )
    
    if cfg.TRAINER.COOP.PREC in ["fp32", "amp"]:
        clip_model.float()
    
    return clip_model, preprocess


class TextEncoder(nn.Module):
    """æ–‡æœ¬ç¼–ç å™¨ï¼ˆBiomedCLIP çš„ PubMedBERTï¼‰"""
    def __init__(self, clip_model):
        super().__init__()
        self.transformer = clip_model.text.transformer
        self.token_embedding = clip_model.text.token_embedding
        self.positional_embedding = clip_model.text.positional_embedding
        self.ln_final = clip_model.text.ln_final
        self.text_projection = clip_model.text.text_projection
        self.attn_mask = clip_model.text.attn_mask
        self.dtype = clip_model.dtype

    def forward(self, prompts, tokenized_prompts):
        """å‰å‘ä¼ æ’­"""
        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x, attn_mask=self.attn_mask)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)
        
        # æå– [EOS] token ç‰¹å¾
        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection
        return x


class TriplePromptLearner(nn.Module):
    """
    ä¸‰å±‚çº§ Prompt å­¦ä¹ å™¨
    
    åŒ…å«:
    1. é«˜è´¨é‡ Promptï¼ˆæ•™å¸ˆï¼Œå†»ç»“ï¼‰ï¼šGPT-4 ç”Ÿæˆçš„å®Œæ•´ä¸´åºŠæè¿°
    2. ä¸­ç­‰è´¨é‡ Promptï¼ˆå­¦ç”Ÿ1ï¼Œå¯å­¦ä¹ ï¼‰ï¼šCUSTOM_BIOMEDDPT_TEMPLATES å›ºå®šæ¨¡æ¿
    3. ä½è´¨é‡ Promptï¼ˆå­¦ç”Ÿ2ï¼Œå¯å­¦ä¹ ï¼‰ï¼šç±»åˆ«åæˆ–ç©ºæç¤º
    """
    def __init__(self, cfg, classnames, clip_model):
        super().__init__()
        self.cfg = cfg
        self.classnames = classnames
        self.n_cls = len(classnames)
        self.n_ctx = cfg.TRAINER.COOP.N_CTX
        self.dtype = clip_model.dtype
        self.ctx_dim = clip_model.text.ln_final.weight.shape[0]
        self.dataset_name = cfg.DATASET.NAME
        
        print("\n" + "="*80)
        print("ğŸš€ åˆå§‹åŒ–ä¸‰å±‚çº§ Prompt å­¦ä¹ å™¨")
        print("="*80)
        
        # ========== 1. é«˜è´¨é‡ Promptï¼ˆæ•™å¸ˆï¼Œå†»ç»“ï¼‰==========
        print("\nã€å±‚çº§ 1ã€‘é«˜è´¨é‡ Promptï¼ˆæ•™å¸ˆï¼Œå†»ç»“ï¼‰")
        print("-" * 80)
        self._init_high_quality_prompts(clip_model)
        
        # ========== 2. ä¸­ç­‰è´¨é‡ Promptï¼ˆå­¦ç”Ÿ1ï¼Œå¯å­¦ä¹ ï¼‰==========
        print("\nã€å±‚çº§ 2ã€‘ä¸­ç­‰è´¨é‡ Promptï¼ˆå­¦ç”Ÿ1ï¼Œå¯å­¦ä¹ ï¼‰")
        print("-" * 80)
        self._init_medium_quality_prompts(clip_model)
        
        # ========== 3. ä½è´¨é‡ Promptï¼ˆå­¦ç”Ÿ2ï¼Œå¯å­¦ä¹ ï¼‰==========
        print("\nã€å±‚çº§ 3ã€‘ä½è´¨é‡ Promptï¼ˆå­¦ç”Ÿ2ï¼Œå¯å­¦ä¹ ï¼‰")
        print("-" * 80)
        self._init_low_quality_prompts(clip_model)
        
        print("\n" + "="*80)
        print("âœ… ä¸‰å±‚çº§ Prompt åˆå§‹åŒ–å®Œæˆ")
        print("="*80 + "\n")

    def _init_high_quality_prompts(self, clip_model):
        """åˆå§‹åŒ–é«˜è´¨é‡ Promptï¼ˆGPT-4 ç”Ÿæˆï¼‰"""
        high_quality_prompts = []
        
        for cls in self.classnames:
            if self.dataset_name in TEMPLATES and cls in TEMPLATES[self.dataset_name]:
                # ä½¿ç”¨ç¬¬ä¸€æ¡ GPT-4 æè¿°
                prompt = TEMPLATES[self.dataset_name][cls][0]
                high_quality_prompts.append(prompt)
            else:
                print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° {self.dataset_name}/{cls} çš„ GPT-4 Promptï¼Œä½¿ç”¨é»˜è®¤")
                high_quality_prompts.append(f"a medical image of {cls}")
        
        # åˆ†è¯å’ŒåµŒå…¥
        self.high_quality_tokenized = torch.cat([
            tokenize([p], context_length=77) for p in high_quality_prompts
        ])
        
        with torch.no_grad():
            high_quality_embedding = clip_model.text.token_embedding(
                self.high_quality_tokenized
            ).type(self.dtype)
        
        # å†»ç»“é«˜è´¨é‡ Promptï¼ˆä¸å‚ä¸è®­ç»ƒï¼‰
        self.register_buffer("high_quality_prompts", high_quality_embedding)
        
        print(f"âœ… åŠ è½½ {self.n_cls} ä¸ªé«˜è´¨é‡ Promptï¼ˆGPT-4 ç”Ÿæˆï¼‰")
        print(f"ç¤ºä¾‹: {high_quality_prompts[0][:65]}...")

    def _init_medium_quality_prompts(self, clip_model):
        """åˆå§‹åŒ–ä¸­ç­‰è´¨é‡ Promptï¼ˆCUSTOM_BIOMEDDPT_TEMPLATESï¼‰"""
        # è·å–æ•°æ®é›†å¯¹åº”çš„æ¨¡æ¿
        if self.dataset_name in CUSTOM_BIOMEDDPT_TEMPLATES:
            template = CUSTOM_BIOMEDDPT_TEMPLATES[self.dataset_name]
        else:
            print(f"âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ° {self.dataset_name} çš„ä¸­ç­‰è´¨é‡æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤")
            template = "a medical image of a {}."
        
        # ç”Ÿæˆä¸­ç­‰è´¨é‡ Prompt
        medium_quality_prompts = [template.format(cls) for cls in self.classnames]
        
        print(f"ä½¿ç”¨æ¨¡æ¿: {template}")
        print(f"ç”Ÿæˆçš„ä¸­ç­‰è´¨é‡ Prompt ç¤ºä¾‹:")
        for cls, prompt in zip(self.classnames[:3], medium_quality_prompts[:3]):
            print(f"  {cls:15} -> {prompt}")
        
        # ä½¿ç”¨æ¨¡æ¿åˆå§‹åŒ–å¯å­¦ä¹ å‘é‡
        init_text = template.replace("{}", self.classnames[0])
        prompt = tokenize([init_text], context_length=77)[0]
        with torch.no_grad():
            embedding = clip_model.text.token_embedding(prompt).type(self.dtype)
        
        # æå–æ¨¡æ¿éƒ¨åˆ†ï¼ˆå»æ‰ç±»åˆ«åï¼‰
        # ä¾‹å¦‚ "a MR photo of a" éƒ¨åˆ†
        init_words = init_text.split(self.classnames[0])[0].strip().split()
        n_ctx_actual = min(self.n_ctx, len(init_words))
        ctx_vectors_med = embedding[1: 1 + n_ctx_actual, :]
        
        # å¡«å……åˆ°æŒ‡å®šé•¿åº¦
        if n_ctx_actual < self.n_ctx:
            padding = torch.zeros(
                self.n_ctx - n_ctx_actual, self.ctx_dim, dtype=self.dtype
            )
            ctx_vectors_med = torch.cat([ctx_vectors_med, padding], dim=0)
        
        # å¯å­¦ä¹ çš„ä¸­ç­‰è´¨é‡ä¸Šä¸‹æ–‡å‘é‡
        self.ctx_medium = nn.Parameter(ctx_vectors_med)
        
        # æ„é€ ä¸­ç­‰è´¨é‡ Prompt çš„å›ºå®šéƒ¨åˆ†
        self.medium_quality_tokenized = torch.cat([
            tokenize([p], context_length=77) for p in medium_quality_prompts
        ])
        
        with torch.no_grad():
            medium_quality_embedding = clip_model.text.token_embedding(
                self.medium_quality_tokenized
            ).type(self.dtype)
        
        self.register_buffer("token_prefix_med", medium_quality_embedding[:, :1, :])
        self.register_buffer("token_suffix_med", medium_quality_embedding[:, 1 + self.n_ctx:, :])
        
        print(f"âœ… ä¸­ç­‰è´¨é‡ Prompt åˆå§‹åŒ–å®Œæˆï¼Œå¯å­¦ä¹ å‚æ•°: {self.ctx_medium.numel()}")

    def _init_low_quality_prompts(self, clip_model):
        """åˆå§‹åŒ–ä½è´¨é‡ Promptï¼ˆç±»åˆ«åæˆ–ç©ºæç¤ºï¼‰"""
        # è·å–ä½è´¨é‡æ¨¡æ¿ç±»å‹
        low_template_type = self.cfg.TRAINER.BIOMEDTRIPLEPROMPT.LOW_TEMPLATE_TYPE
        
        if low_template_type not in ZERO_SHOT_TEMPLATES:
            print(f"âš ï¸  è­¦å‘Š: æœªçŸ¥æ¨¡æ¿ç±»å‹ '{low_template_type}'ï¼Œä½¿ç”¨ 'minimal'")
            low_template_type = "minimal"
        
        template = ZERO_SHOT_TEMPLATES[low_template_type]
        print(f"ä½¿ç”¨ä½è´¨é‡æ¨¡æ¿: {low_template_type}")
        
        # ç”Ÿæˆä½è´¨é‡ Prompt
        if template == "":
            low_quality_prompts = ["" for _ in self.classnames]
            print("ä½¿ç”¨ç©ºå­—ç¬¦ä¸²ä½œä¸ºä½è´¨é‡ Prompt")
        else:
            low_quality_prompts = [template.format(**{"class": cls}) for cls in self.classnames]
            print(f"ç”Ÿæˆçš„ä½è´¨é‡ Prompt ç¤ºä¾‹:")
            for cls, prompt in zip(self.classnames[:3], low_quality_prompts[:3]):
                print(f"  {cls:15} -> '{prompt}'")
        
        # åˆå§‹åŒ–å¯å­¦ä¹ å‘é‡
        if low_quality_prompts[0] == "":
            # ç©ºå­—ç¬¦ä¸²ï¼šéšæœºåˆå§‹åŒ–
            print("ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆç©ºæç¤ºï¼‰")
            ctx_vectors_low = torch.empty(self.n_ctx, self.ctx_dim, dtype=self.dtype)
            nn.init.normal_(ctx_vectors_low, std=0.02)
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½è´¨é‡ Prompt ç¼–ç 
            init_text = low_quality_prompts[0]
            prompt = tokenize([init_text], context_length=77)[0]
            with torch.no_grad():
                embedding = clip_model.text.token_embedding(prompt).type(self.dtype)
            
            init_words = init_text.split()
            n_ctx_actual = min(self.n_ctx, len(init_words))
            ctx_vectors_low = embedding[1: 1 + n_ctx_actual, :]
            
            # å¡«å……
            if n_ctx_actual < self.n_ctx:
                padding = torch.zeros(
                    self.n_ctx - n_ctx_actual, self.ctx_dim, dtype=self.dtype
                )
                ctx_vectors_low = torch.cat([ctx_vectors_low, padding], dim=0)
        
        # å¯å­¦ä¹ çš„ä½è´¨é‡ä¸Šä¸‹æ–‡å‘é‡
        self.ctx_low = nn.Parameter(ctx_vectors_low)
        
        # æ„é€ ä½è´¨é‡ Prompt çš„å›ºå®šéƒ¨åˆ†
        low_quality_full = [f"{p} ." if p else "X ." for p in low_quality_prompts]
        self.low_quality_tokenized = torch.cat([
            tokenize([p], context_length=77) for p in low_quality_full
        ])
        
        with torch.no_grad():
            low_quality_embedding = clip_model.text.token_embedding(
                self.low_quality_tokenized
            ).type(self.dtype)
        
        self.register_buffer("token_prefix_low", low_quality_embedding[:, :1, :])
        self.register_buffer("token_suffix_low", low_quality_embedding[:, 1 + self.n_ctx:, :])
        
        print(f"[OK] Low-quality Prompt initializedï¼Œå¯å­¦ä¹ å‚æ•°: {self.ctx_low.numel()}")

    def forward(self):
        """
        è¿”å›ä¸‰å±‚çº§ Prompt åµŒå…¥
        
        è¿”å›:
            high_quality_prompts: é«˜è´¨é‡ Promptï¼ˆå†»ç»“ï¼‰
            medium_quality_prompts: ä¸­ç­‰è´¨é‡ Promptï¼ˆå¯å­¦ä¹ ï¼‰
            low_quality_prompts: ä½è´¨é‡ Promptï¼ˆå¯å­¦ä¹ ï¼‰
        """
        # 1. é«˜è´¨é‡ Promptï¼ˆå†»ç»“ï¼Œç›´æ¥è¿”å›ï¼‰
        high_quality_prompts = self.high_quality_prompts
        
        # 2. ä¸­ç­‰è´¨é‡ Promptï¼ˆå¯å­¦ä¹ ï¼‰
        ctx_med = self.ctx_medium
        if ctx_med.dim() == 2:
            ctx_med = ctx_med.unsqueeze(0).expand(self.n_cls, -1, -1)
        
        prefix_med = self.token_prefix_med
        suffix_med = self.token_suffix_med
        medium_quality_prompts = torch.cat([prefix_med, ctx_med, suffix_med], dim=1)
        
        # 3. ä½è´¨é‡ Promptï¼ˆå¯å­¦ä¹ ï¼‰
        ctx_low = self.ctx_low
        if ctx_low.dim() == 2:
            ctx_low = ctx_low.unsqueeze(0).expand(self.n_cls, -1, -1)
        
        prefix_low = self.token_prefix_low
        suffix_low = self.token_suffix_low
        low_quality_prompts = torch.cat([prefix_low, ctx_low, suffix_low], dim=1)
        
        return high_quality_prompts, medium_quality_prompts, low_quality_prompts


class CustomCLIP(nn.Module):
    """ä¸‰è·¯å¾„ CLIP æ¨¡å‹"""
    def __init__(self, cfg, classnames, clip_model):
        super().__init__()
        self.prompt_learner = TriplePromptLearner(cfg, classnames, clip_model)
        self.image_encoder = clip_model.visual
        self.text_encoder = TextEncoder(clip_model)
        self.logit_scale = clip_model.logit_scale
        self.dtype = clip_model.dtype
        
        self.high_quality_tokenized = self.prompt_learner.high_quality_tokenized
        self.medium_quality_tokenized = self.prompt_learner.medium_quality_tokenized
        self.low_quality_tokenized = self.prompt_learner.low_quality_tokenized

    def forward(self, image):
        """
        å‰å‘ä¼ æ’­
        
        è¿”å›:
            logits_high: é«˜è´¨é‡ Prompt çš„ logits
            logits_medium: ä¸­ç­‰è´¨é‡ Prompt çš„ logits
            logits_low: ä½è´¨é‡ Prompt çš„ logits
        """
        # æå–å›¾åƒç‰¹å¾
        image_features = self.image_encoder(image.type(self.dtype))
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        
        # è·å–ä¸‰å±‚çº§ Prompt
        high_prompts, medium_prompts, low_prompts = self.prompt_learner()
        
        # ç¼–ç é«˜è´¨é‡ Prompt
        text_features_high = self.text_encoder(high_prompts, self.high_quality_tokenized)
        text_features_high = text_features_high / text_features_high.norm(dim=-1, keepdim=True)
        
        # ç¼–ç ä¸­ç­‰è´¨é‡ Prompt
        text_features_medium = self.text_encoder(medium_prompts, self.medium_quality_tokenized)
        text_features_medium = text_features_medium / text_features_medium.norm(dim=-1, keepdim=True)
        
        # ç¼–ç ä½è´¨é‡ Prompt
        text_features_low = self.text_encoder(low_prompts, self.low_quality_tokenized)
        text_features_low = text_features_low / text_features_low.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = self.logit_scale.exp()
        logits_high = logit_scale * image_features @ text_features_high.t()
        logits_medium = logit_scale * image_features @ text_features_medium.t()
        logits_low = logit_scale * image_features @ text_features_low.t()
        
        return logits_high, logits_medium, logits_low


@TRAINER_REGISTRY.register()
class BiomedTriplePrompt(TrainerX):
    """
    BiomedTriplePrompt è®­ç»ƒå™¨
    
    ä¸‰å±‚çº§æ¢¯åº¦å¼çŸ¥è¯†è’¸é¦:
    1. é«˜è´¨é‡ï¼ˆæ•™å¸ˆï¼‰â†’ ä¸­ç­‰è´¨é‡ï¼ˆå­¦ç”Ÿ1ï¼‰ï¼šç»†ç²’åº¦è¯­ä¹‰ä¼ é€’
    2. é«˜è´¨é‡ï¼ˆæ•™å¸ˆï¼‰â†’ ä½è´¨é‡ï¼ˆå­¦ç”Ÿ2ï¼‰ï¼šå¼ºåˆ°å¼±çš„é²æ£’æ€§å­¦ä¹ 
    3. ä¸­ç­‰è´¨é‡ï¼ˆå­¦ç”Ÿ1ï¼‰â†’ ä½è´¨é‡ï¼ˆå­¦ç”Ÿ2ï¼‰ï¼šä¸­é—´å±‚è¯­ä¹‰æ¡¥æ¥
    
    æŸå¤±å‡½æ•°:
    L = L_ce_med + L_ce_low + 
        Î»1 * L_kd(highâ†’med) + Î»2 * L_kd(highâ†’low) + Î»3 * L_kd(medâ†’low) +
        Î»4 * L_align(high, med) + Î»5 * L_align(high, low)
    """
    
    def check_cfg(self, cfg):
        assert cfg.TRAINER.COOP.PREC in ["fp16", "fp32", "amp"]

    def build_model(self):
        """æ„å»ºä¸‰è·¯å¾„æ¨¡å‹"""
        cfg = self.cfg
        classnames = self.dm.dataset.classnames
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ æ„å»º BiomedTriplePrompt æ¨¡å‹")
        print(f"{'='*80}")
        
        # åŠ è½½ BiomedCLIP
        clip_model, _ = load_biomedclip_to_cpu(cfg)
        
        # æ„å»ºä¸‰è·¯å¾„æ¨¡å‹
        self.model = CustomCLIP(cfg, classnames, clip_model)
        self.model.to(self.device)
        
        # åªä¼˜åŒ–ä¸­ç­‰è´¨é‡å’Œä½è´¨é‡ Prompt çš„å‚æ•°
        print("\nğŸ¯ å¯è®­ç»ƒå‚æ•°:")
        for name, param in self.model.named_parameters():
            if "ctx_medium" in name or "ctx_low" in name:
                print(f"  âœ… {name}: {param.shape}")
            else:
                param.requires_grad = False
        
        # ä¼˜åŒ–å™¨
        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)
        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)
        self.register_model("prompt_learner", self.model.prompt_learner, self.optim, self.sched)
        
        self.scaler = GradScaler() if cfg.TRAINER.COOP.PREC == "amp" else None
        
        print(f"\n{'='*80}\n")

    def forward_backward(self, batch):
        """
        è®­ç»ƒæ­¥éª¤
        
        æŸå¤±å‡½æ•°:
        L = L_ce_med + L_ce_low + 
            Î»1 * L_kd(highâ†’med) + Î»2 * L_kd(highâ†’low) + Î»3 * L_kd(medâ†’low) +
            Î»4 * L_align(high, med) + Î»5 * L_align(high, low)
        """
        image, label = self.parse_batch_train(batch)
        
        # å‰å‘ä¼ æ’­
        logits_high, logits_medium, logits_low = self.model(image)
        
        # ========== æŸå¤± 1 & 2ï¼šä¸­ç­‰å’Œä½è´¨é‡è·¯å¾„çš„äº¤å‰ç†µæŸå¤± ==========
        loss_ce_medium = F.cross_entropy(logits_medium, label)
        loss_ce_low = F.cross_entropy(logits_low, label)
        
        # ========== æŸå¤± 3ï¼šçŸ¥è¯†è’¸é¦ï¼ˆé«˜è´¨é‡ â†’ ä¸­ç­‰è´¨é‡ï¼‰==========
        T = self.cfg.TRAINER.BIOMEDTRIPLEPROMPT.T
        loss_kd_high_to_med = F.kl_div(
            F.log_softmax(logits_medium / T, dim=1),
            F.softmax(logits_high.detach() / T, dim=1),
            reduction='batchmean'
        ) * (T ** 2)
        
        # ========== æŸå¤± 4ï¼šçŸ¥è¯†è’¸é¦ï¼ˆé«˜è´¨é‡ â†’ ä½è´¨é‡ï¼‰==========
        loss_kd_high_to_low = F.kl_div(
            F.log_softmax(logits_low / T, dim=1),
            F.softmax(logits_high.detach() / T, dim=1),
            reduction='batchmean'
        ) * (T ** 2)
        
        # ========== æŸå¤± 5ï¼šçŸ¥è¯†è’¸é¦ï¼ˆä¸­ç­‰è´¨é‡ â†’ ä½è´¨é‡ï¼‰==========
        loss_kd_med_to_low = F.kl_div(
            F.log_softmax(logits_low / T, dim=1),
            F.softmax(logits_medium.detach() / T, dim=1),
            reduction='batchmean'
        ) * (T ** 2)
        
        # ========== æŸå¤± 6 & 7ï¼šç‰¹å¾å¯¹é½æŸå¤± ==========
        high_prompts, medium_prompts, low_prompts = self.model.prompt_learner()
        
        loss_align_high_med = F.mse_loss(medium_prompts, high_prompts.detach())
        loss_align_high_low = F.mse_loss(low_prompts, high_prompts.detach())
        
        # ========== æ€»æŸå¤± ==========
        cfg_tp = self.cfg.TRAINER.BIOMEDTRIPLEPROMPT
        
        loss = (
            loss_ce_medium + loss_ce_low +
            cfg_tp.LAMBDA_KD_HIGH_MED * loss_kd_high_to_med +
            cfg_tp.LAMBDA_KD_HIGH_LOW * loss_kd_high_to_low +
            cfg_tp.LAMBDA_KD_MED_LOW * loss_kd_med_to_low +
            cfg_tp.LAMBDA_ALIGN_HIGH_MED * loss_align_high_med +
            cfg_tp.LAMBDA_ALIGN_HIGH_LOW * loss_align_high_low
        )
        
        # åå‘ä¼ æ’­
        self.model_backward_and_update(loss)
        
        # è®°å½•æŸå¤±
        loss_summary = {
            "loss": loss.item(),
            "loss_ce_med": loss_ce_medium.item(),
            "loss_ce_low": loss_ce_low.item(),
            "loss_kd_h2m": loss_kd_high_to_med.item(),
            "loss_kd_h2l": loss_kd_high_to_low.item(),
            "loss_kd_m2l": loss_kd_med_to_low.item(),
            "loss_align_hm": loss_align_high_med.item(),
            "loss_align_hl": loss_align_high_low.item(),
            "acc_high": compute_accuracy(logits_high, label)[0].item(),
            "acc_med": compute_accuracy(logits_medium, label)[0].item(),
            "acc_low": compute_accuracy(logits_low, label)[0].item(),
        }
        
        if (self.batch_idx + 1) == self.num_batches:
            self.update_lr()
        
        return loss_summary

    def parse_batch_train(self, batch):
        """è§£æè®­ç»ƒæ‰¹æ¬¡"""
        input = batch["img"].to(self.device)
        label = batch["label"].to(self.device)
        return input, label

    @torch.no_grad()
    def test(self, split=None, trainer=None):
        """
        æµ‹è¯•ï¼ˆä½¿ç”¨ä½è´¨é‡ Promptï¼‰
        
        æœ€ç»ˆç›®æ ‡ï¼šè®©ä½è´¨é‡ Prompt è¾¾åˆ°æ¥è¿‘é«˜è´¨é‡ Prompt çš„æ€§èƒ½
        """
        self.set_model_mode("eval")
        self.evaluator.reset()
        
        if split is None:
            split = self.cfg.TEST.SPLIT
        
        if split == "val" and self.val_loader is not None:
            data_loader = self.val_loader
        else:
            split = "test"
            data_loader = self.test_loader
        
        print(f"ğŸ§ª æµ‹è¯•ä½è´¨é‡ Prompt æ€§èƒ½ï¼ˆ{split} splitï¼‰")
        
        for batch_idx, batch in enumerate(data_loader):
            input, label = self.parse_batch_test(batch)
            _, _, logits_low = self.model(input)  # åªä½¿ç”¨ä½è´¨é‡ Prompt
            self.evaluator.process(logits_low, label)
        
        results = self.evaluator.evaluate()
        
        for k, v in results.items():
            tag = f"{split}/{k}"
            self.write_scalar(tag, v, self.epoch)
        
        return list(results.values())[0]
